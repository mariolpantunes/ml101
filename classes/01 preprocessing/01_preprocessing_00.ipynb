{"cells":[{"cell_type":"markdown","metadata":{"id":"5ZCd_iiWmsT4"},"source":["# ML 101\n","\n","This notebook contains the common methods to do dataset pre-processing, cleaning and normalization."]},{"cell_type":"code","source":["%pip install matplotlib pandas numpy"],"metadata":{"id":"q059on14mvFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYlWRNEImsT7"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"7Op1K_FXmsT-"},"source":["## Loading a sample dataset\n","\n","Let us consider a toy dataset with only four features:\n","1. Country (String)\n","2. Age (Int)\n","3. Salary (Int)\n","4. Purchased (Yes/No)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODVDfRZpmsT-"},"outputs":[],"source":["# import dataset\n","df = pd.read_csv('https://raw.githubusercontent.com/mariolpantunes/ml101/main/datasets/data_prep.csv')\n","# print the first rows of the dataset\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RssLNZKkmsT_"},"outputs":[],"source":["# print the last rows of the dataset\n","df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MecLGWs4msUA"},"outputs":[],"source":["# viewing statistical info about dataset\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"dm6EAbadmsUB"},"source":["The dataset may contain duplicated rows due to any error on the acquisition."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSuiZ99UmsUB"},"outputs":[],"source":["# dropping duplicate values\n","#print(df.duplicated())\n","\n","duplicate_rows = df[df.duplicated()]\n","print(f'{duplicate_rows}')\n","df = df.drop_duplicates()\n","duplicate_rows = df[df.duplicated()]\n","print(f'{duplicate_rows}')\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"tByibqcPmsUC"},"source":["## Missing Data\n","\n","Another common issue is the presence of missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBdk0E3JmsUD"},"outputs":[],"source":["# checking for missing values\n","# checking the number of missing data\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"4Ap3Sc6SmsUE"},"source":["### Missing Data on categorical fields\n","\n","There are two approaches:\n","1. Drop the rows with missing values\n","2. Replace them with the most frequent element"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdGscXlSmsUF"},"outputs":[],"source":["# Dropping categorical data rows with missing values\n","#df.dropna(how='any', subset=['Country', 'Purchased'], inplace=True)\n","# Replace null with the most frequent in that class\n","ax = df[['Country']].value_counts().plot(kind='barh')\n","plt.show()\n","ax = df[['Purchased']].value_counts().plot(kind='barh')\n","plt.show()\n","df['Country'] = df['Country'].fillna(df['Country'].value_counts().index[0])\n","df['Purchased'] = df['Purchased'].fillna(df['Purchased'].value_counts().index[0])\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"va-ljve9msUG"},"source":["## Split the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL5321vfmsUG"},"outputs":[],"source":["# Splitting dataset into independent and dependent variable\n","X = df[['Country', 'Age', 'Salary']].values\n","y = df['Purchased'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2xIREuymsUH"},"outputs":[],"source":["print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjtUk7TVmsUI"},"outputs":[],"source":["print(y)"]},{"cell_type":"markdown","metadata":{"id":"DSOZuy-emsUJ"},"source":["### Replace Missing numerial data\n","\n","We will use the capabilities of Scikit-learn to deal with missing values in the numerical fields."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQgy0D4rmsUJ"},"outputs":[],"source":["# replacing the missing values in the age and salary column with the mean\n","# import the SimpleImputer class from the sklearn library\n","from sklearn.impute import SimpleImputer\n","# help(SimpleImputer)\n","print(X[:, 1:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJ7ikCDUmsUK"},"outputs":[],"source":["imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","imputer.fit(X[:, 1:3])\n","X[:, 1:3] = imputer.transform(X[:, 1:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhTT5BgLmsUK"},"outputs":[],"source":["print(X[:, 1:3])"]},{"cell_type":"markdown","metadata":{"id":"6GINhUH4msUL"},"source":["## Convert Categorical Data\n","\n","The optimization process can not handle categorical data.\n","There are two possibilities:\n","1. **Label enconding**: Label encoding is simply converting each value in a column to a number.\n","2. **One Hot enconding**: The basic strategy is to convert each category value into a new column and assigns a 1 or 0 (True/False) value to the column. \n","\n","Label encoding has the advantage that it is straightforward but it has the disadvantage that the numeric values can be “misinterpreted” by the algorithms.\n","For example, the value of 0 is obviously less than the value of 4 but does that really correspond to the data set in real life.\n","One Hot Encoding has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBjwmo4XmsUL"},"outputs":[],"source":["# Handling Categorical Data\n","# One Hot Encoding\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","ct = ColumnTransformer(transformers=[('enconder', OneHotEncoder(), [0])], remainder='passthrough')\n","X = np.array(ct.fit_transform(X))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"wz9xGwaqmsUM"},"outputs":[],"source":["print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBNQf9cvmsUM"},"outputs":[],"source":["print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ER-AphkLmsUM"},"outputs":[],"source":["# Encoding the target variable\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y = le.fit_transform(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGORuEIvmsUN"},"outputs":[],"source":["print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thkqgXkymsUN"},"outputs":[],"source":["# Splitting Dataset into Training and Test Set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4glYFDimsUO"},"outputs":[],"source":["print(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ws3gGSUjmsUO"},"outputs":[],"source":["print(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIj3MmBGmsUO"},"outputs":[],"source":["print(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7r4KScL3msUO"},"outputs":[],"source":["print(y_test)"]},{"cell_type":"markdown","metadata":{"id":"aGwBOAEGmsUP"},"source":["## Data Scaling and Normalization \n","\n","One of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that, in scaling, you're changing the range of your data while in normalization you're changing the shape of the distribution of your data. Let's talk a little more in-depth about each of these options.\n","\n","### Scaling\n","\n","This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points, like support vector machines, or SVM or k-nearest neighbors, or KNN. With these algorithms, a change of \"1\" in any numeric feature is given the same importance.\n","\n","For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n","\n","By scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in just a second, this is just to help illustrate my point.)\n","\n","### Normalization\n","\n","Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n","\n",">[Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution): Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n","\n","In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8X_Xjy8omsUP"},"outputs":[],"source":["# Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train[:, 4:] = sc.fit_transform(X_train[:, 4:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xendHH8rmsUQ"},"outputs":[],"source":["print(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"320uPOohmsUQ"},"outputs":[],"source":["print(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoPBHEpDmsUR"},"outputs":[],"source":["X_test[:, 4:] = sc.transform(X_test[:, 4:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JmHGgfvmsUR"},"outputs":[],"source":["print(X_test)"]},{"cell_type":"markdown","source":["# Test a classifier\n","\n","Train a linear classifier using the previsouly processed dataset."],"metadata":{"id":"a6ST0R0G7xhb"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import matthews_corrcoef\n","\n","#clf = LogisticRegression(random_state=2).fit(X_train, y_train)\n","clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)\n","preds = clf.predict(X_test)\n","mcc = matthews_corrcoef(y_test, preds)\n","print(mcc)"],"metadata":{"id":"uj_EtwHx8im9"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"colab":{"name":"01_preprocess_00.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}