{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_models_01.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOL57mxypOniWlT+tJN+dQW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ML 101\n","\n","## Regression\n","\n","Regression belongs to the class of [Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning) tasks where the datasets that are used for predictive/statistical modeling contain [continuous labels](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/discrete-vs-continuous-variables/). But, let's define a regression problem more mathematically.\n","\n","Let's consider the following image below:\n","\n"," ![regression](https://raw.githubusercontent.com/mariolpantunes/ml101/main/figs/regression.png)\n","\n","So, in the above image, $X$ is the set of values that correspond to the  the space of input values and $y$ is the output variables but note that these values are predicted by $h$. $h$ is the function that maps the $X$ values to $y$ (often called as predictor). For historical reasons, this $h$ is referred to as a hypothesis function.\n","\n","Note that, the predicted values here are continuous in nature. So, your ultimate goal is, given a training set, to learn a function $h:X \\rightarrow Y$ so that $h(x)$ is a \"good\" predictor for the corresponding value of $y$. Also, keep in mind that the domain of values that both $X$ and $Y$ accept are all real numbers and you can define it like this: $X=Y=\\mathbb{R}$ where, $\\mathbb{R}$ is the set of all real numbers.\n","\n","A pair $(x^i, y^i)$ is called a training example. You can define the training set as $\\{(x^i, y^i) ; i = 1,...,m\\}$ (in case the training set contains $m$ instances and there is only one feature $x$ in the dataset).\n","\n","A bit of mathematics there for you so that you don't go wrong even in the simplest of things. So, according to Han, Kamber, and Pei-\n","\n","> In general, these methods are used to predict the value of a response (dependent) variable from one or more predictor (independent) variables, where the variables are numeric.\" - [Data Mining: Concepts and Techniques (3rd edn.)](https://www.sciencedirect.com/book/9780123814791/data-mining-concepts-and-techniques)\n","\n"],"metadata":{"id":"747VmLRVwt4N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJV2I9ZzvkRa"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = [15, 8]\n","import seaborn as sns\n","import numpy as np\n","import math"]},{"cell_type":"code","source":["#create linear dataset\n","x = np.arange(start=1, stop=20, step=1)\n","x_long = np.arange(start=1, stop=30, step=1)\n","y_linear = x*2+1\n","y_poly = x**2+2*x+1\n","y_exp = math.e**x\n","y_pow = 2**x \n","y_log = np.log(x)\n","y_sin = np.sin(x)\n","\n","yys = [y_linear, y_poly, y_exp, y_pow, y_log, y_sin]\n","titles = ['Linear', 'Polynomial', 'Exponential', 'Power', 'Logarithm', 'Periodic']\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","plt.show()"],"metadata":{"id":"VBd3y9Rj4Be5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n","\n","LinearRegression fits a linear model with coefficients $w = (w_1,..., w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."],"metadata":{"id":"5hRYTQWA97f1"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","\n","  reg = LinearRegression().fit(x.reshape(-1, 1), y)\n","  y_hat = reg.predict(x_long.reshape(-1, 1))\n","  axs[r][c].plot(x_long, y_hat, 'ro')\n","\n","plt.show()"],"metadata":{"id":"Bf8CnnIZ9-Yt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html)\n","\n","Compared to the OLS (ordinary least squares) estimator, the coefficient weights are slightly shifted toward zeros, which stabilises them.\n","\n","As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian.\n","\n","The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.\n","\n","We also plot predictions and uncertainties for Bayesian Ridge Regression for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.\n"],"metadata":{"id":"HGTTxMeKxDK2"}},{"cell_type":"code","source":["from sklearn.linear_model import BayesianRidge\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","\n","  reg = BayesianRidge().fit(x.reshape(-1, 1), y)\n","  y_hat = reg.predict(x_long.reshape(-1, 1))\n","  axs[r][c].plot(x_long, y_hat, 'ro')\n","\n","plt.show()"],"metadata":{"id":"vpoMg2rSxz-1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n","\n","Support Vector Machines (SVMs) are well known in classification problems. The use of SVMs in regression is not as well documented, however. These types of models are known as Support Vector Regression (SVR).\n","\n","SVR gives us the flexibility to define how much error is acceptable in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data.\n","\n","In contrast to OLS, the objective function of SVR is to minimize the coefficients — more specifically, the l2-norm of the coefficient vector — not the squared error. The error term is instead handled in the constraints, where we set the absolute error less than or equal to a specified margin, called the maximum error, ϵ (epsilon).\n","\n","*italicized text*"],"metadata":{"id":"qKWReTj7zUkZ"}},{"cell_type":"code","source":["from sklearn.svm import SVR\n","\n","kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n","\n","for k in kernels:\n","  print(f'Kernel: {k}')\n","  fig, axs = plt.subplots(2, 3)\n","\n","  for i in range(len(yys)):\n","    y = yys[i]\n","    r = int(i/3)\n","    c = int(i%3)\n","\n","    axs[r][c].plot(x, y)\n","    axs[r][c].set_title(titles[i])\n","\n","    reg = SVR(kernel=k).fit(x.reshape(-1, 1), y)\n","    y_hat = reg.predict(x_long.reshape(-1, 1))\n","    axs[r][c].plot(x_long, y_hat, 'ro')\n","  plt.show()"],"metadata":{"id":"qhgScWwkzcgh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Neural Network](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n","\n","\n"],"metadata":{"id":"YM6-WH0M4Sq2"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPRegressor\n","\n","activation = ['logistic', 'tanh', 'relu']\n","\n","for a in activation:\n","  print(f'Activation: {a}')\n","  fig, axs = plt.subplots(2, 3)\n","\n","  for i in range(len(yys)):\n","    y = yys[i]\n","    r = int(i/3)\n","    c = int(i%3)\n","\n","    axs[r][c].plot(x, y)\n","    axs[r][c].set_title(titles[i])\n","\n","    reg = MLPRegressor(activation=a, max_iter=1000, hidden_layer_sizes=(200,)).fit(x.reshape(-1, 1), y)\n","    y_hat = reg.predict(x_long.reshape(-1, 1))\n","    axs[r][c].plot(x_long, y_hat, 'ro')\n","  plt.show()"],"metadata":{"id":"JSOySPcy4yeV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n","\n","KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood. The size of the neighbourhood needs to be set by the analyst or can be chosen using cross-validation (we will see this later) to select the size that minimises the mean-squared error.\n","\n","While the method is quite appealing, it quickly becomes impractical when the dimension increases, i.e., when there are many independent variables."],"metadata":{"id":"aRY4fmdk6tFw"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsRegressor\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","\n","  reg = KNeighborsRegressor(n_neighbors=1).fit(x.reshape(-1, 1), y)\n","  y_hat = reg.predict(x_long.reshape(-1, 1))\n","  axs[r][c].plot(x_long, y_hat, 'ro')\n","\n","plt.show()\n"],"metadata":{"id":"B6oV3FJn6tZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decision Trees\n","\n","Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n","\n","The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data. "],"metadata":{"id":"4Vdj4XRqAc-S"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","\n","  reg = DecisionTreeRegressor(max_depth=None).fit(x.reshape(-1, 1), y)\n","  y_hat = reg.predict(x_long.reshape(-1, 1))\n","  axs[r][c].plot(x_long, y_hat, 'ro')\n","\n","plt.show()"],"metadata":{"id":"n8mzxC3QB94A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n","\n","Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.\n","\n","A Random Forest operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees.\n","\n","A Random Forest Regression model is powerful and accurate. It usually performs great on many problems, including features with non-linear relationships. Disadvantages, however, include the following: there is no interpretability, overfitting may easily occur, we must choose the number of trees to include in the model.\n"],"metadata":{"id":"7td97Bh6ClR7"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","\n","  reg = RandomForestRegressor(n_estimators=200).fit(x.reshape(-1, 1), y)\n","  y_hat = reg.predict(x_long.reshape(-1, 1))\n","  axs[r][c].plot(x_long, y_hat, 'ro')\n","\n","plt.show()"],"metadata":{"id":"tWMIcFCdDZZ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n","\n","GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function."],"metadata":{"id":"tK2zVquJR28O"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","\n","  reg = GradientBoostingRegressor(n_estimators=200).fit(x.reshape(-1, 1), y)\n","  y_hat = reg.predict(x_long.reshape(-1, 1))\n","  axs[r][c].plot(x_long, y_hat, 'ro')\n","\n","plt.show()"],"metadata":{"id":"L9wEmmgXSZVv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Voting Ensemble](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html)\n","\n","Prediction voting regressor for unfitted estimators.\n","\n","A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.\n","\n","The idea behind the VotingRegressor is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.\n","\n","\n"],"metadata":{"id":"1sstJ5CqPhxB"}},{"cell_type":"code","source":["from sklearn.ensemble import VotingRegressor\n","\n","r1 = LinearRegression()\n","r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n","\n","fig, axs = plt.subplots(2, 3)\n","\n","for i in range(len(yys)):\n","  y = yys[i]\n","  r = int(i/3)\n","  c = int(i%3)\n","\n","  axs[r][c].plot(x, y)\n","  axs[r][c].set_title(titles[i])\n","\n","  reg = VotingRegressor([('lr', r1), ('rf', r2)]).fit(x.reshape(-1, 1), y)\n","  y_hat = reg.predict(x_long.reshape(-1, 1))\n","  axs[r][c].plot(x_long, y_hat, 'ro')\n","\n","plt.show()"],"metadata":{"id":"pmOe7F7XPvBy"},"execution_count":null,"outputs":[]}]}