{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06_tm_graph.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN1yZEt7JmQiKZLBdobPGZH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ML 101\n","\n","## Text Mining\n","\n","In this project, we will explore a simplified version of the DPWC model to draw a graph of similar personalities.\n","\n","The data will be consumed from Twitter, and for each search, we create a DWP profile with the most relevant words.\n","\n","After we apply a Matrix Factorization to estimate the \"real\" value of the word with zero weight.\n","\n","Finally, we use a thresholding method to create the entries on the graph."],"metadata":{"id":"WTvhIdEILOpF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"W16qJeogxbbe"},"outputs":[],"source":["!pip install pyvis gensim python-twitter git+git://github.com/mariolpantunes/nmf@main#egg=nmf git+git://github.com/mariolpantunes/uts@main#egg=uts --upgrade"]},{"cell_type":"code","source":["import pprint\n","import twitter\n","import numpy as np\n","\n","from IPython.core.display import display, HTML\n","from pyvis.network import Network\n","\n","import uts.thresholding as thres\n","\n","import nltk\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import RSLPStemmer\n","from nltk.tokenize import RegexpTokenizer\n","\n","from nmf.nmf import nmf_mu\n","\n","import math\n","\n","pp = pprint.PrettyPrinter(indent=2)\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('rslp')"],"metadata":{"id":"X66LmyIZ3UR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = RegexpTokenizer(r'\\w+')\n","stop_words = set(stopwords.words('portuguese'))\n","stop_words.update(set(stopwords.words('english')))\n","stop_words.add('https')\n","stemmer = RSLPStemmer()\n","\n","\n","db = {}\n","\n","\n","def cosine_similarity(a,b):\n","  return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n","\n","def generate_ngrams(tokens, n=2):\n","    ngrams = zip(*[tokens[i:] for i in range(n)])\n","    return [\" \".join(ngram) for ngram in ngrams]\n","\n","\n","def tokenize(text):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [stemmer.stem(w.lower()) for w in tokens if not w in stop_words and w.isalpha() and len(w) > 3]\n","    #ngrams = generate_ngrams(tokens)\n","    #return tokens + ngrams\n","    return tokens\n","\n","\n","def get_term_frequency(corpus, p=0.2):\n","  tf = {}\n","  # count the terms\n","  for t in corpus:\n","    if t not in tf:\n","      tf[t] = 0\n","    tf[t]+=1\n","  \n","  # discard non-relevant items\n","  neighborhood = [(k, v) for k, v in tf.items() if v > 1] \n","  neighborhood.sort(key=lambda tup: tup[1], reverse=True)\n","  limit = int(len(neighborhood)*p)\n","  neighborhood = neighborhood[:limit]\n","\n","  # return \n","  return neighborhood"],"metadata":{"id":"jFUqvRM_3e5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["api = twitter.Api(consumer_key='2lDgkNXdm03bxodf55vlY5IHo',\n","                  consumer_secret='w5SaNzPCLyaBL1ieyGpm4uwjan5Y2GDqQjbbSUoBTT5Fl3cLP4',\n","                  access_token_key='276620312-0oyEjiC76ouJXCWALH5P9L3NXHSQ7kPw75jL9wse',\n","                  access_token_secret='HuJgudHMikT6VGd13M79GkXf0IdzDw20xyePaM8gHRJgg')\n","\n","terms = [\"António Guterres\", \"Aníbal Cavaco Silva\",\n","    \"Mário Soares\",\n","    \"Pedro Passos Coelho\",\n","    \"José Manuel Durão Barroso\",\n","    \"José Sócrates\",\n","    \"Pedro Santana Lopes\"]\n","\n","for t in terms:\n","  if t not in db:\n","    results = api.GetSearch(term=t, count=300, lang='pt')\n","    corpus = []\n","    for r in results:\n","      corpus.extend(tokenize(r.text.lower()))\n","    tf = get_term_frequency(corpus)\n","    db[t] = tf\n","\n","pp.pprint(f'{db}')"],"metadata":{"id":"OWmBDlBB7tqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create vector matrix\n","vocab = set()\n","\n","for k in db:\n","  tf = db[k]\n","  for t,_ in tf:\n","    vocab.add(t)\n","\n","X = np.zeros((len(db), len(vocab)))\n","\n","r = 0\n","for k in db:\n","  tf = db[k]\n","  c = 0\n","  for t, v in tf:\n","    vocab.add(t)\n","    c += 1\n","    X[r,c] = v\n","  r +=1\n","\n","rows, cols = X.shape\n","k = int(math.ceil(rows/2.0))\n","Xr, W, H, cost = nmf_mu(X, k=k, seed=42)\n","\n","seeds = [3, 5, 7, 11, 13]\n","for s in seeds:\n","  Xt, Wt, Ht, costt = nmf_mu(X, k=k, seed=s)\n","  if costt < cost:\n","    cost = costt\n","    Xr = Xt\n","\n","# compute the distance matrix (graph)\n","D = np.identity(len(db))\n","for i in range(0, len(Xr)):\n","  for j in range(0, len(Xr)):\n","    similarity = cosine_similarity(Xr[i], Xr[j])\n","    D[i][j] = similarity\n","    D[j][i] = similarity\n","\n","# compute the ideal threshold\n","flat_distance = D.ravel()\n","#t = thres.isodata(flat_distance)\n","t = np.percentile(flat_distance, 75)\n","\n","D[D<t] = 0"],"metadata":{"id":"4IFYRgY9dS92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = Network()\n","\n","# Create the nodes\n","i=0\n","for k in db:\n","  net.add_node(i, label=k)\n","  i+=1\n","\n","\n","for i in range(0, len(D)-1):\n","  for j in range(1, len(D)):\n","    if (i!=j) and D[i][j] > 0:\n","      net.add_edge(i, j, weight=D[i][j])\n","\n","\n","net.show('network.html')\n","display(HTML('network.html'))"],"metadata":{"id":"oQHrkE7ndS2u"},"execution_count":null,"outputs":[]}]}