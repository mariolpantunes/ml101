{"cells":[{"cell_type":"markdown","metadata":{"id":"VNRgTyR65Ien"},"source":["# Attack and anomaly detection in IoT sensors in IoT sites using machine learning approaches\n","\n","Article information:\n","* Authors: Mahmudul Hasan, Milon Islam, Ishrak Islam Zarif and M.M.A.Hashem\n","* Publication: 20 May 2019\n","* DOI: https://doi.org/10.1016/j.iot.2019.100059\n","\n","## Objective\n","\n","In this notebook we test different approaches to encode categorical values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJEl0fPt5Ies"},"outputs":[],"source":["# import numpy and pandas to load the dataset\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","\n","# graphics\n","from pylab import rcParams\n","%matplotlib inline \n","rcParams['figure.figsize'] = 8, 8\n","\n","# load dataset\n","df = pd.read_csv('https://media.githubusercontent.com/media/mariolpantunes/ml101/main/datasets/mainSimulationAccessTraces.csv',  error_bad_lines=False, header = 0)"]},{"cell_type":"markdown","metadata":{"id":"rjb_n_st5Ieu"},"source":["## Data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zrShf0L5Ieu"},"outputs":[],"source":["# Fix missing Accessed Node Type\n","df = df.fillna({'accessedNodeType': 'Malicious'})\n","\n","# Fix the malformated values in column Value\n","df = df.fillna({'value': 0.0})\n","replace_values = {'true': 1.0, 'false': 0.0, 'twenty': 20.0, 'none': 0.0}\n","df = df.replace({'value':replace_values})\n","\n","# Fix other errors not mentioned on the paper\n","df.loc[df.value.str.contains('^org').fillna(False), 'value'] = df[df.value.str.contains('^org').fillna(False)]['value'].apply(lambda x: int(x.split('@')[1],16))\n","\n","# Replace the names of the target variable to be consistent with the paper\n","replace_values={'anomalous(DoSattack)':'DoS', 'anomalous(scan)': 'SC', 'anomalous(malitiousControl)':'M.C',\n","               'anomalous(malitiousOperation)': 'M.O', 'anomalous(spying)':'SP', 'anomalous(dataProbing)':'D.P',\n","               'anomalous(wrongSetUp)':'W.S', 'normal':'NL'}\n","df = df.replace({'normality':replace_values})\n","\n","# Check for NaN values\n","missing = df.isnull().values.any()\n","print('Missing Values ? {}'.format(missing))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2G_EKnh5Iev"},"outputs":[],"source":["# Convert timestamp to datetime\n","time = pd.to_datetime(df['timestamp'], unit='ms')\n","df['timestamp'] = time\n","\n","# Sort the samples by the timestamp\n","df.sort_values('timestamp')\n","\n","# Drop the non-relevant feature\n","df = df.drop('timestamp', 1)\n","\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"YNrNS_li5Iew"},"source":["## Encoding categorical values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDueffkv5Iew"},"outputs":[],"source":["print('Dataset datatypes (before transformation):')\n","print(df.dtypes)\n","\n","# Convert value to float\n","df['value'] = pd.to_numeric(df['value'])\n","\n","# Convert target variable to categorical\n","df['normality'] = df['normality'].astype('category')\n","\n","# Apply label enconding to the remaining columns\n","# The label encoding was applied directly with pandas\n","columns=['sourceID', 'sourceAddress', 'sourceType','sourceLocation',\n","'destinationServiceAddress', 'destinationServiceType', 'destinationLocation', \n","'accessedNodeAddress', 'accessedNodeType', 'operation']\n","\n","# Ordinal encoder (label encoding)\n","for column in columns:\n","    df[column] = df[column].astype('category')\n","    df[column] = df[column].cat.codes\n","\n","# OneHot encoder\n","dfDummies = pd.get_dummies(df, columns=columns, prefix=columns).drop(['value', 'normality'], 1)\n","df = df.drop(columns, 1).join(dfDummies)\n","\n","print('------------------------------------------------------')\n","print('Dataset datatypes (after transformation):')\n","print(df.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"vR6UNFVN5Iex"},"source":["## Dataset convertion"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"fdXyKrBw5Iey"},"outputs":[],"source":["# Import the preprocessing libraries\n","from sklearn import preprocessing\n","from sklearn.metrics import confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import mutual_info_classif\n","from sklearn.feature_selection import SelectKBest\n","\n","# Convert pandas dataframe X and y\n","le = preprocessing.LabelEncoder()\n","le.fit(df.normality)\n","y = le.transform(df.normality)\n","df_y = df['normality'].copy()\n","df = df.drop('normality', 1)\n","X = df.copy()\n","\n","#Scale only the numerical feature (value)\n","scaler = StandardScaler()\n","feature = X[['value']]\n","scaler.fit(feature.values)\n","feature = scaler.transform(feature.values)\n","X.loc[:, 'value'] = feature"]},{"cell_type":"markdown","metadata":{"id":"agJV5Mss5Iez"},"source":["## Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_tlf0dq5Iez"},"outputs":[],"source":["import math\n","\n","# Helper functions to compute performance metrics\n","def decompose_cm(cm, c=0):\n","    TP = cm[c,c]\n","    tmp = np.delete(np.delete(cm, c, 0), c, 1)\n","    TN = np.sum(tmp)\n","    FP = np.sum(cm[c, :]) - TP\n","    FN = np.sum(cm[:, c]) - TP\n","    return TP, TN, FP, FN\n","\n","\n","def compute_performance_metrics(tp, tn, fp, fn):\n","    mcc = float(tp*tn - fp*fn) / math.sqrt(float(tp+fp)*float(tp+fn)*float(tn+fp)*float(tn+fn))\n","    if (tp+tn) == 0.0:\n","        return 0,0,0,0,mcc\n","    else:\n","        acc = (tp+tn)/(tp+tn+fp+fn)\n","        if tp == 0.0:\n","            return acc, 0, 0, 0, mcc\n","        else:\n","            pre = tp/(tp+fp)\n","            rec = tp/(tp+fn)\n","            f1 = (2*tp)/(2*tp + fp + fn)\n","            \n","    return acc, pre, rec, f1, mcc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW-lTnZQ5Ie0"},"outputs":[],"source":["# Library used to speedup computation\n","from joblib import parallel_backend\n","\n","# Import the necessary models\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","# Import the stratified KFold library\n","from sklearn.model_selection import StratifiedKFold\n","\n","# RoC related libraries\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import roc_auc_score\n","\n","# Prepare the dataset to be used in 5-fold cross validation\n","skf = StratifiedKFold(n_splits=5)\n","\n","# List of models names and models algorithms\n","models = [\n","    ('LR',  SGDClassifier(loss='log')),\n","    ('SVM', SGDClassifier(loss='hinge')),\n","    ('DT',  DecisionTreeClassifier()),\n","    ('RF',  RandomForestClassifier()),\n","    ('ANN', MLPClassifier())]\n","\n","# Dictionary that will contain the confusion matrix for each model\n","cm = {}\n","\n","# Dictionary that will contain the RoC scores\n","roc = {}\n","\n","# fold counter\n","k=0\n","\n","# FPR for ROC curve\n","mean_fpr = np.linspace(0, 1, 100)\n","\n","# Use the parallel backend to speedup the fit operation\n","with parallel_backend('threading'):\n","    \n","    # For each fold\n","    for train_index, test_index in skf.split(X, y):\n","        k += 1\n","        print('K-Fold: {}'.format(k))\n","    \n","        #X_train, X_test = X[train_index], X[test_index]\n","        X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n","        y_train, y_test = y[train_index], y[test_index]\n","        #y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","        \n","    \n","        # For each model\n","        for model in models:\n","            print('\\tModel: {}'.format(model[0]))\n","            model[1].fit(X_train, y_train)\n","            y_pred = model[1].predict(X_test)\n","            \n","            # Store the information for the confusion matrix\n","            if model[0] in cm:\n","                cm[model[0]] = np.add(cm[model[0]], confusion_matrix(y_test, y_pred))\n","            else:\n","                cm[model[0]] = confusion_matrix(y_test, y_pred)\n","            \n","            # Store the information for the RoC curve\n","            tpr = []\n","            \n","            for i in range (0, len(cm[model[0]])):\n","                f, t, _ = roc_curve(y_test, y_pred, pos_label=i)\n","                r = auc(f, t)\n","                interp_tpr = np.interp(mean_fpr, f, t)\n","                interp_tpr[0] = 0.0\n","                tpr.append(interp_tpr)\n","            \n","            if model[0] in roc:\n","                roc[model[0]]['tpr'].append(tpr)\n","            else:\n","                d = {}\n","                d['tpr'] = [tpr]\n","                roc[model[0]] = d\n","\n","# Compute the mean values for TPR\n","for model in models:\n","    d = roc[model[0]]\n","    d['tpr'] = np.array(d['tpr']).mean(0)"]},{"cell_type":"markdown","metadata":{"id":"yPDxDYm25Ie0"},"source":["## Comparison Table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_XyuzQm5Ie1"},"outputs":[],"source":["print('Evaluation  Classifiers')\n","print('Metrics     LR    SVM   DT    RF    ANN')\n","\n","accuracy = []\n","precision = []\n","recall = []\n","f1 = []\n","mcc = []\n","\n","# For all models\n","for model in models:\n","    acc_score = pre_score = rec_score = f1_score = mcc_score = 0\n","    \n","    # For all classes\n","    for i in range(0, len(cm[model[0]])):\n","        tp, tn, fp, fn = decompose_cm(cm[model[0]], i)\n","        a, p, r, f, m  = compute_performance_metrics(tp, tn, fp, fn)\n","        acc_score += a\n","        pre_score += p\n","        rec_score += r\n","        f1_score  += f\n","        mcc_score += m\n","    \n","    # Compute macro average\n","    acc_score /= len(cm[model[0]])\n","    pre_score /= len(cm[model[0]])\n","    rec_score /= len(cm[model[0]])\n","    f1_score  /= len(cm[model[0]])\n","    mcc_score /= len(cm[model[0]])\n","    \n","    #\n","    accuracy.append(acc_score)\n","    precision.append(pre_score)\n","    recall.append(rec_score)\n","    f1.append(f1_score)\n","    mcc.append(mcc_score)\n","\n","print('Accuracy    {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*accuracy))\n","print('Precision   {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*precision))\n","print('Recall      {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*recall))\n","print('F1 score    {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*f1))\n","print('MCC         {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*mcc))"]},{"cell_type":"markdown","metadata":{"id":"N2Iz1vYd5Ie1"},"source":["## Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"SnaiFfTJ5Ie1"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","# For all models\n","labels = le.inverse_transform(list(range(0, len(cm[model[0]]))))\n","for model in models:\n","    # Normalise\n","    normalized_cm = cm[model[0]].astype('float') / cm[model[0]].sum(axis=1)[:, np.newaxis]\n","    disp = ConfusionMatrixDisplay(confusion_matrix=normalized_cm, display_labels=labels)\n","    disp.plot(cmap='cividis')"]},{"cell_type":"markdown","metadata":{"id":"mcL1KEMt5Ie2"},"source":["## Conclusion\n","\n","In this notebook we tried two different encodings for categorical data: ordinal (similar to label encoding in scikitlearn) and one hot.\n","Again, we used the MCC to evalute them.\n","\n","The best values of MCC were obtained with the one hot encoding scheme."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLWYwkP-5Ie2"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"bonus01.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}