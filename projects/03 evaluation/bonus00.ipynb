{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack and anomaly detection in IoT sensors in IoT sites using machine learning approaches\n",
    "\n",
    "Article information:\n",
    "* Authors: Mahmudul Hasan, Milon Islam, Ishrak Islam Zarif and M.M.A.Hashem\n",
    "* Publication: 20 May 2019\n",
    "* DOI: https://doi.org/10.1016/j.iot.2019.100059\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this notebook we test under sampling of the majority class to balanced the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas to load the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# graphics\n",
    "from pylab import rcParams\n",
    "%matplotlib inline \n",
    "rcParams['figure.figsize'] = 8, 8\n",
    "\n",
    "# load dataset\n",
    "dataset_file = 'dataset/mainSimulationAccessTraces.csv'\n",
    "df = pd.read_csv(dataset_file, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing Accessed Node Type\n",
    "df = df.fillna({'accessedNodeType': 'Malicious'})\n",
    "\n",
    "# Fix the malformated values in column Value\n",
    "df = df.fillna({'value': 0.0})\n",
    "replace_values = {'true': 1.0, 'false': 0.0, 'twenty': 20.0, 'none': 0.0}\n",
    "df = df.replace({'value':replace_values})\n",
    "\n",
    "# Fix other errors not mentioned on the paper\n",
    "df.loc[df.value.str.contains('^org').fillna(False), 'value'] = df[df.value.str.contains('^org').fillna(False)]['value'].apply(lambda x: int(x.split('@')[1],16))\n",
    "\n",
    "# Replace the names of the target variable to be consistent with the paper\n",
    "replace_values={'anomalous(DoSattack)':'DoS', 'anomalous(scan)': 'SC', 'anomalous(malitiousControl)':'M.C',\n",
    "               'anomalous(malitiousOperation)': 'M.O', 'anomalous(spying)':'SP', 'anomalous(dataProbing)':'D.P',\n",
    "               'anomalous(wrongSetUp)':'W.S', 'normal':'NL'}\n",
    "df = df.replace({'normality':replace_values})\n",
    "\n",
    "# Check for NaN values\n",
    "missing = df.isnull().values.any()\n",
    "print('Missing Values ? {}'.format(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "time = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "df['timestamp'] = time\n",
    "\n",
    "# Sort the samples by the timestamp\n",
    "df.sort_values('timestamp')\n",
    "\n",
    "# Drop the non-relevant feature\n",
    "df = df.drop('timestamp', 1)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample\n",
    "\n",
    "As stated previously the dataset is quite uneven.\n",
    "For that reason we will apply a resampling strategy to improve the ratio of the dataset.\n",
    "We selected random under-sampling that reduces the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into normal and anomaly \n",
    "df_normal  = df[df['normality'] == 'NL']\n",
    "df_anomaly = df[df['normality'] != 'NL']\n",
    "\n",
    "# Count the total number of anomalies\n",
    "anomaly_count = df_anomaly.normality.value_counts().sum()\n",
    "\n",
    "f=2.0\n",
    "\n",
    "# Random re-sample the normal part to be factor x number of anomalies\n",
    "df_normal  = df_normal.sample(int(f*anomaly_count))\n",
    "\n",
    "# Merge the two dataframes\n",
    "df = pd.concat([df_normal, df_anomaly], axis=0)\n",
    "\n",
    "df.normality.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset datatypes (before transformation):')\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert value to float\n",
    "df['value'] = pd.to_numeric(df['value'])\n",
    "\n",
    "# Convert target variable to categorical\n",
    "df['normality'] = df['normality'].astype('category')\n",
    "\n",
    "# Apply label enconding to the remaining columns\n",
    "# The label encoding was applied directly with pandas\n",
    "columns=['sourceID', 'sourceAddress', 'sourceType','sourceLocation',\n",
    "'destinationServiceAddress', 'destinationServiceType', 'destinationLocation', \n",
    "'accessedNodeAddress', 'accessedNodeType', 'operation']\n",
    "\n",
    "for column in columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "    df[column] = df[column].cat.codes\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "print('Dataset datatypes (after transformation):')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the preprocessing libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Convert pandas dataframe X and y\n",
    "columns=['sourceID', 'sourceAddress', 'sourceType', 'sourceLocation', \n",
    "'destinationServiceAddress', 'destinationServiceType', 'destinationLocation',\n",
    "'accessedNodeAddress', 'accessedNodeType', 'operation', 'value']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.normality)\n",
    "y = le.transform(df.normality)\n",
    "X = df[columns].copy()\n",
    "\n",
    "#Scale only the numerical feature (value)\n",
    "scaler = StandardScaler()\n",
    "feature = X[['value']]\n",
    "scaler.fit(feature.values)\n",
    "feature = scaler.transform(feature.values)\n",
    "X.loc[:, 'value'] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Helper functions to compute performance metrics\n",
    "def decompose_cm(cm, c=0):\n",
    "    TP = cm[c,c]\n",
    "    tmp = np.delete(np.delete(cm, c, 0), c, 1)\n",
    "    TN = np.sum(tmp)\n",
    "    FP = np.sum(cm[c, :]) - TP\n",
    "    FN = np.sum(cm[:, c]) - TP\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def compute_performance_metrics(tp, tn, fp, fn):\n",
    "    mcc = mcc = float(tp*tn - fp*fn) / math.sqrt(float(tp+fp)*float(tp+fn)*float(tn+fp)*float(tn+fn))\n",
    "    if (tp+tn) == 0.0:\n",
    "        return 0,0,0,0,mcc\n",
    "    else:\n",
    "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        if tp == 0.0:\n",
    "            return acc, 0, 0, 0, mcc\n",
    "        else:\n",
    "            pre = tp/(tp+fp)\n",
    "            rec = tp/(tp+fn)\n",
    "            f1 = (2*tp)/(2*tp + fp + fn)\n",
    "            \n",
    "    return acc, pre, rec, f1, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library used to speedup computation\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Import the necessary models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Import the stratified KFold library\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# RoC related libraries\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Prepare the dataset to be used in 5-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# List of models names and models algorithms\n",
    "models = [\n",
    "    ('LR',  SGDClassifier(loss='log', class_weight='balanced')),\n",
    "    ('SVM', SGDClassifier(loss='hinge', class_weight='balanced')),\n",
    "    ('DT',  DecisionTreeClassifier(class_weight='balanced')),\n",
    "    ('RF',  RandomForestClassifier(class_weight='balanced')),\n",
    "    ('ANN', MLPClassifier())\n",
    "]\n",
    "\n",
    "# Dictionary that will contain the confusion matrix for each model\n",
    "cm = {}\n",
    "\n",
    "# Dictionary that will contain the RoC scores\n",
    "roc = {}\n",
    "\n",
    "# fold counter\n",
    "k=0\n",
    "\n",
    "# FPR for ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Use the parallel backend to speedup the fit operation\n",
    "with parallel_backend('threading'):\n",
    "    \n",
    "    # For each fold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        k += 1\n",
    "        print('K-Fold: {}'.format(k))\n",
    "    \n",
    "        #X_train, X_test = X[train_index], X[test_index]\n",
    "        X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    \n",
    "        # For each model\n",
    "        for model in models:\n",
    "            print('\\tModel: {}'.format(model[0]))\n",
    "            model[1].fit(X_train, y_train)\n",
    "            y_pred = model[1].predict(X_test)\n",
    "            \n",
    "            # Store the information for the confusion matrix\n",
    "            if model[0] in cm:\n",
    "                cm[model[0]] = np.add(cm[model[0]], confusion_matrix(y_test, y_pred))\n",
    "            else:\n",
    "                cm[model[0]] = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Store the information for the RoC curve\n",
    "            tpr = []\n",
    "            \n",
    "            for i in range (0, len(cm[model[0]])):\n",
    "                f, t, _ = roc_curve(y_test, y_pred, pos_label=i)\n",
    "                r = auc(f, t)\n",
    "                interp_tpr = np.interp(mean_fpr, f, t)\n",
    "                interp_tpr[0] = 0.0\n",
    "                tpr.append(interp_tpr)\n",
    "            \n",
    "            if model[0] in roc:\n",
    "                roc[model[0]]['tpr'].append(tpr)\n",
    "            else:\n",
    "                d = {}\n",
    "                d['tpr'] = [tpr]\n",
    "                roc[model[0]] = d\n",
    "\n",
    "# Compute the mean values for TPR\n",
    "for model in models:\n",
    "    d = roc[model[0]]\n",
    "    d['tpr'] = np.array(d['tpr']).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluation  Classifiers')\n",
    "print('Metrics     LR    SVM   DT    RF    ANN')\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "mcc = []\n",
    "\n",
    "# For all models\n",
    "for model in models:\n",
    "    acc_score = pre_score = rec_score = f1_score = mcc_score = 0\n",
    "    \n",
    "    # For all classes\n",
    "    for i in range(0, len(cm[model[0]])):\n",
    "        tp, tn, fp, fn = decompose_cm(cm[model[0]], i)\n",
    "        a, p, r, f, m  = compute_performance_metrics(tp, tn, fp, fn)\n",
    "        acc_score += a\n",
    "        pre_score += p\n",
    "        rec_score += r\n",
    "        f1_score  += f\n",
    "        mcc_score += m\n",
    "    \n",
    "    # Compute macro average\n",
    "    acc_score /= len(cm[model[0]])\n",
    "    pre_score /= len(cm[model[0]])\n",
    "    rec_score /= len(cm[model[0]])\n",
    "    f1_score  /= len(cm[model[0]])\n",
    "    mcc_score /= len(cm[model[0]])\n",
    "    \n",
    "    #\n",
    "    accuracy.append(acc_score)\n",
    "    precision.append(pre_score)\n",
    "    recall.append(rec_score)\n",
    "    f1.append(f1_score)\n",
    "    mcc.append(mcc_score)\n",
    "\n",
    "print('Accuracy    {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*accuracy))\n",
    "print('Precision   {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*precision))\n",
    "print('Recall      {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*recall))\n",
    "print('F1 score    {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*f1))\n",
    "print('MCC         {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# For all models\n",
    "labels = le.inverse_transform(list(range(0, len(cm[model[0]]))))\n",
    "for model in models:\n",
    "    # Normalise\n",
    "    normalized_cm = cm[model[0]].astype('float') / cm[model[0]].sum(axis=1)[:, np.newaxis]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=normalized_cm, display_labels=labels)\n",
    "    disp.plot(cmap='cividis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we added a step to balance the dataset.\n",
    "We implemented a under sampling of the majority class, in this case the normal state. The undersampling was performed as followns:\n",
    "1. Count the number of anomalies (#NA)\n",
    "2. Random sample facto * #NA normal labelled samples\n",
    "\n",
    "Furthermore, we added the following option to all the possible models: `class_weight='balanced'`\n",
    "\n",
    "We tested 3 different factors ([1.0, 2.0, 3.0]) and added the MCC score as a metric to better evalute the performance between the classifiers.\n",
    "\n",
    "The best values of MCC were obtained with a factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
