{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack and anomaly detection in IoT sensors in IoT sites using machine learning approaches\n",
    "\n",
    "Article information:\n",
    "* Authors: Mahmudul Hasan, Milon Islam, Ishrak Islam Zarif and M.M.A.Hashem\n",
    "* Publication: 20 May 2019\n",
    "* DOI: https://doi.org/10.1016/j.iot.2019.100059\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this notebook we apply the previous improvemtns plus feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas to load the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# graphics\n",
    "from pylab import rcParams\n",
    "%matplotlib inline \n",
    "rcParams['figure.figsize'] = 8, 8\n",
    "\n",
    "# load dataset\n",
    "dataset_file = 'dataset/mainSimulationAccessTraces.csv'\n",
    "df = pd.read_csv(dataset_file, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing Accessed Node Type\n",
    "df = df.fillna({'accessedNodeType': 'Malicious'})\n",
    "\n",
    "# Fix the malformated values in column Value\n",
    "df = df.fillna({'value': 0.0})\n",
    "replace_values = {'true': 1.0, 'false': 0.0, 'twenty': 20.0, 'none': 0.0}\n",
    "df = df.replace({'value':replace_values})\n",
    "\n",
    "# Fix other errors not mentioned on the paper\n",
    "df.loc[df.value.str.contains('^org').fillna(False), 'value'] = df[df.value.str.contains('^org').fillna(False)]['value'].apply(lambda x: int(x.split('@')[1],16))\n",
    "\n",
    "# Replace the names of the target variable to be consistent with the paper\n",
    "replace_values={'anomalous(DoSattack)':'DoS', 'anomalous(scan)': 'SC', 'anomalous(malitiousControl)':'M.C',\n",
    "               'anomalous(malitiousOperation)': 'M.O', 'anomalous(spying)':'SP', 'anomalous(dataProbing)':'D.P',\n",
    "               'anomalous(wrongSetUp)':'W.S', 'normal':'NL'}\n",
    "df = df.replace({'normality':replace_values})\n",
    "\n",
    "# Check for NaN values\n",
    "missing = df.isnull().values.any()\n",
    "print('Missing Values ? {}'.format(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "time = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "df['timestamp'] = time\n",
    "\n",
    "# Sort the samples by the timestamp\n",
    "df.sort_values('timestamp')\n",
    "\n",
    "# Drop the non-relevant feature\n",
    "df = df.drop('timestamp', 1)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into normal and anomaly \n",
    "df_normal  = df[df['normality'] == 'NL']\n",
    "df_anomaly = df[df['normality'] != 'NL']\n",
    "\n",
    "# Count the total number of anomalies\n",
    "anomaly_count = df_anomaly.normality.value_counts().sum()\n",
    "\n",
    "f=2.0\n",
    "\n",
    "# Random re-sample the normal part to be factor x number of anomalies\n",
    "df_normal  = df_normal.sample(int(f*anomaly_count))\n",
    "\n",
    "# Merge the two dataframes\n",
    "df = pd.concat([df_normal, df_anomaly], axis=0)\n",
    "\n",
    "df.normality.value_counts().plot(kind='bar', title='Count (target)')\n",
    "\n",
    "# Check for NaN values\n",
    "missing = df.isnull().values.any()\n",
    "print('Missing Values ? {}'.format(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset datatypes (before transformation):')\n",
    "print(df.dtypes)\n",
    "print(df.shape)\n",
    "\n",
    "# Convert value to float\n",
    "df['value'] = pd.to_numeric(df['value'])\n",
    "\n",
    "# Convert target variable to categorical\n",
    "df['normality'] = df['normality'].astype('category')\n",
    "\n",
    "# Apply label enconding to the remaining columns\n",
    "# The label encoding was applied directly with pandas\n",
    "columns=['sourceID', 'sourceAddress', 'sourceType','sourceLocation',\n",
    "'destinationServiceAddress', 'destinationServiceType', 'destinationLocation', \n",
    "'accessedNodeAddress', 'accessedNodeType', 'operation']\n",
    "\n",
    "# OneHot encoder\n",
    "dfDummies = pd.get_dummies(df, columns=columns, prefix=columns).drop(['value', 'normality'], 1)\n",
    "df = df.drop(columns, 1).join(dfDummies)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "print('Dataset datatypes (after transformation):')\n",
    "print(df.dtypes)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "missing = df.isnull().values.any()\n",
    "print('Missing Values ? {}'.format(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "### Mutual Information \n",
    "\n",
    "We performed feature selection based on Mutual Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the preprocessing libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Convert pandas dataframe X and y\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.normality)\n",
    "y = le.transform(df.normality)\n",
    "df_y = df['normality'].copy()\n",
    "df = df.drop('normality', 1)\n",
    "X = df.copy()\n",
    "\n",
    "#Scale only the numerical feature (value)\n",
    "scaler = StandardScaler()\n",
    "feature = X[['value']]\n",
    "scaler.fit(feature.values)\n",
    "feature = scaler.transform(feature.values)\n",
    "X.loc[:, 'value'] = feature\n",
    "\n",
    "# Value selected visualy with the plot\n",
    "k=17\n",
    "\n",
    "# Feature selection based on Mutual Information\n",
    "fit = SelectKBest(score_func=mutual_info_classif, k=17).fit(X, y)\n",
    "X = fit.transform(X)\n",
    "\n",
    "#dfscores = pd.DataFrame(fit.scores_)\n",
    "#dfcolumns = pd.DataFrame(X.columns)\n",
    "#featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "#featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "#nlargest = featureScores.nlargest(X.shape[1],'Score')\n",
    "#print(nlargest.head(30))   #print K best features\n",
    "# Plot the feature rank\n",
    "#plt.figure()\n",
    "#plt.bar(nlargest['Specs'], nlargest['Score'],  width = 0.4) \n",
    "#plt.title('Feature Ranking(Mutual Information)')\n",
    "#plt.xticks(rotation='vertical')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Helper functions to compute performance metrics\n",
    "def decompose_cm(cm, c=0):\n",
    "    TP = cm[c,c]\n",
    "    tmp = np.delete(np.delete(cm, c, 0), c, 1)\n",
    "    TN = np.sum(tmp)\n",
    "    FP = np.sum(cm[c, :]) - TP\n",
    "    FN = np.sum(cm[:, c]) - TP\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def compute_performance_metrics(tp, tn, fp, fn):\n",
    "    d = math.sqrt(float(tp+fp)*float(tp+fn)*float(tn+fp)*float(tn+fn))\n",
    "    if d == 0:\n",
    "        mcc = 0\n",
    "    else:\n",
    "        mcc = float(tp*tn - fp*fn) / d\n",
    "    if (tp+tn) == 0.0:\n",
    "        return 0,0,0,0,mcc\n",
    "    else:\n",
    "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        if tp == 0.0:\n",
    "            return acc, 0, 0, 0, mcc\n",
    "        else:\n",
    "            pre = tp/(tp+fp)\n",
    "            rec = tp/(tp+fn)\n",
    "            f1 = (2*tp)/(2*tp + fp + fn)\n",
    "            \n",
    "    return acc, pre, rec, f1, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library used to speedup computation\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Import the necessary models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Import the stratified KFold library\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# RoC related libraries\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Prepare the dataset to be used in 5-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# List of models names and models algorithms\n",
    "models = [\n",
    "    ('LR',  SGDClassifier(loss='log', class_weight='balanced')),\n",
    "    ('SVM', SGDClassifier(loss='hinge', class_weight='balanced')),\n",
    "    ('DT',  DecisionTreeClassifier(class_weight='balanced')),\n",
    "    ('RF',  RandomForestClassifier(class_weight='balanced')),\n",
    "    ('ANN', MLPClassifier())\n",
    "]\n",
    "\n",
    "# Dictionary that will contain the confusion matrix for each model\n",
    "cm = {}\n",
    "\n",
    "# Dictionary that will contain the RoC scores\n",
    "roc = {}\n",
    "\n",
    "# fold counter\n",
    "k=0\n",
    "\n",
    "# FPR for ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Use the parallel backend to speedup the fit operation\n",
    "with parallel_backend('threading'):\n",
    "    \n",
    "    # For each fold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        k += 1\n",
    "        print('K-Fold: {}'.format(k))\n",
    "    \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        #X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    \n",
    "        # For each model\n",
    "        for model in models:\n",
    "            print('\\tModel: {}'.format(model[0]))\n",
    "            model[1].fit(X_train, y_train)\n",
    "            y_pred = model[1].predict(X_test)\n",
    "            \n",
    "            # Store the information for the confusion matrix\n",
    "            if model[0] in cm:\n",
    "                cm[model[0]] = np.add(cm[model[0]], confusion_matrix(y_test, y_pred))\n",
    "            else:\n",
    "                cm[model[0]] = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Store the information for the RoC curve\n",
    "            tpr = []\n",
    "            \n",
    "            for i in range (0, len(cm[model[0]])):\n",
    "                f, t, _ = roc_curve(y_test, y_pred, pos_label=i)\n",
    "                r = auc(f, t)\n",
    "                interp_tpr = np.interp(mean_fpr, f, t)\n",
    "                interp_tpr[0] = 0.0\n",
    "                tpr.append(interp_tpr)\n",
    "            \n",
    "            if model[0] in roc:\n",
    "                roc[model[0]]['tpr'].append(tpr)\n",
    "            else:\n",
    "                d = {}\n",
    "                d['tpr'] = [tpr]\n",
    "                roc[model[0]] = d\n",
    "\n",
    "# Compute the mean values for TPR\n",
    "for model in models:\n",
    "    d = roc[model[0]]\n",
    "    d['tpr'] = np.array(d['tpr']).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluation  Classifiers')\n",
    "print('Metrics     LR    SVM   DT    RF    ANN')\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "mcc = []\n",
    "\n",
    "# For all models\n",
    "for model in models:\n",
    "    acc_score = pre_score = rec_score = f1_score = mcc_score = 0\n",
    "    \n",
    "    # For all classes\n",
    "    for i in range(0, len(cm[model[0]])):\n",
    "        tp, tn, fp, fn = decompose_cm(cm[model[0]], i)\n",
    "        a, p, r, f, m  = compute_performance_metrics(tp, tn, fp, fn)\n",
    "        acc_score += a\n",
    "        pre_score += p\n",
    "        rec_score += r\n",
    "        f1_score  += f\n",
    "        mcc_score += m\n",
    "    \n",
    "    # Compute macro average\n",
    "    acc_score /= len(cm[model[0]])\n",
    "    pre_score /= len(cm[model[0]])\n",
    "    rec_score /= len(cm[model[0]])\n",
    "    f1_score  /= len(cm[model[0]])\n",
    "    mcc_score /= len(cm[model[0]])\n",
    "    \n",
    "    #\n",
    "    accuracy.append(acc_score)\n",
    "    precision.append(pre_score)\n",
    "    recall.append(rec_score)\n",
    "    f1.append(f1_score)\n",
    "    mcc.append(mcc_score)\n",
    "\n",
    "print('Accuracy    {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*accuracy))\n",
    "print('Precision   {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*precision))\n",
    "print('Recall      {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*recall))\n",
    "print('F1 score    {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*f1))\n",
    "print('MCC         {:4.3f} {:4.3f} {:4.3f} {:4.3f} {:4.3f}'.format(*mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# For all models\n",
    "labels = le.inverse_transform(list(range(0, len(cm[model[0]]))))\n",
    "for model in models:\n",
    "    # Normalise\n",
    "    normalized_cm = cm[model[0]].astype('float') / cm[model[0]].sum(axis=1)[:, np.newaxis]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=normalized_cm, display_labels=labels)\n",
    "    disp.plot(cmap='cividis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For all models\n",
    "for model in models:\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    tpr = roc[model[0]]['tpr']\n",
    "    # For all classes\n",
    "    for i in range (0, len(cm[model[0]])):\n",
    "        mean_tpr = tpr[i]\n",
    "        # Compute the mean auc based on the mean FPR ant TPR\n",
    "        mean_auc = mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        plt.plot(mean_fpr, mean_tpr,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(le.inverse_transform([i]), mean_auc))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC {}'.format(model[0]))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we applied the previous improvements and added the feature selection.\n",
    "We select mutual information as a method to perform feature selection, since it appears to work well with categorical data.\n",
    "\n",
    "Based on the plot of scores, we selected only the mos relevant 17 features.\n",
    "\n",
    "By applying all the previous improvements we saw an increasing performance with the linear models (LR and SVM), bu a decrease in performance from the non-linear models.\n",
    "\n",
    "That imples that the features that were not selected, altough not having linear correlation with the label, they held enough information for the non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
