{"cells":[{"cell_type":"markdown","metadata":{"id":"XEROkEsVea47"},"source":["# Twitter Project"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mc6UIhSDea5C"},"outputs":[],"source":["!pip install gensim python-twitter matplotlib wordcloud --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsmxnAxkea5G"},"outputs":[],"source":["import twitter\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import FreqDist\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","from gensim.corpora import Dictionary\n","from gensim.models import TfidfModel\n","\n","import matplotlib.pyplot as plt\n","\n","from wordcloud import WordCloud\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","# Define the search term\n","term = 'antonio costa'\n","\n","# Define tokenizer and stop words\n","tokenizer = RegexpTokenizer(r'\\w+')\n","stop_words = set(stopwords.words('portuguese'))\n","stop_words.update(set(stopwords.words('english')))\n","stop_words.add('https')\n","\n","# Add the search term to the stop word list\n","for t in term.split():\n","    stop_words.add(t.lower())\n","\n","def generate_ngrams(tokens, n=2):\n","    ngrams = zip(*[tokens[i:] for i in range(n)])\n","    return [\" \".join(ngram) for ngram in ngrams]\n","    \n","def scores(vectorizer, tfidf_result, limit=10):\n","    scores = zip(vectorizer.get_feature_names(),\n","                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n","    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n","    return sorted_scores[:limit]\n","\n","def display_scores(vectorizer, tfidf_result):\n","    # http://stackoverflow.com/questions/16078015/\n","    scores = zip(vectorizer.get_feature_names(),\n","                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n","    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n","    for item in sorted_scores:\n","        print ('{0:50} Score: {1}'.format(item[0], item[1]))\n","\n","def tokenize(text):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and len(w) > 2]\n","    tokens = [lemmatizer.lemmatize(w) for w in tokens if wordnet.synsets(w)]\n","    bigrams = generate_ngrams(tokens)\n","    return tokens + bigrams\n","    #return tokens\n","\n","api = twitter.Api(consumer_key='2lDgkNXdm03bxodf55vlY5IHo',\n","                  consumer_secret='w5SaNzPCLyaBL1ieyGpm4uwjan5Y2GDqQjbbSUoBTT5Fl3cLP4',\n","                  access_token_key='276620312-0oyEjiC76ouJXCWALH5P9L3NXHSQ7kPw75jL9wse',\n","                  access_token_secret='HuJgudHMikT6VGd13M79GkXf0IdzDw20xyePaM8gHRJgg')\n","\n","results = api.GetSearch(term=term, count=300)\n","\n","corpus = []\n","for r in results:\n","    corpus.append(tokenize(r.text.lower()))\n","    \n","# Build the dictionary\n","dictionary = Dictionary(corpus) \n","\n","# Convert to vector corpus\n","vectors = [dictionary.doc2bow(text) for text in corpus]\n","\n","# Build TF-IDF model\n","tfidf = TfidfModel(vectors)\n","\n","# Get TF-IDF weights\n","weights = [tfidf[v] for v in vectors]\n","weights = [item for sublist in weights for item in sublist]\n","weights = list(set(weights))\n","\n","# Replace term IDs with human consumable strings\n","weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n","weights.sort(key=lambda p: p[1], reverse=True)\n","\n","# Select only k top words\n","weights = weights[:20]\n","\n","# Create the dictionary of frequencies\n","frequencies = {}\n","for p in weights:\n","    frequencies[p[0]] = p[1]\n","\n","# Initialize the cloud\n","wc = WordCloud(\n","    background_color=\"white\",\n","    max_words=2000,\n","    width=1024,\n","    height=720,\n","    stopwords=stop_words\n",")\n","\n","# Generate the cloud\n","wordcloud = wc.generate_from_frequencies(frequencies)\n","\n","# Display the generated image:\n","# the matplotlib way:\n","plt.figure(figsize=(15,15))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()\n","\n","# Display a chart with the distribution of weights from the words\n","x = [v[1] for v in weights]\n","labels = [v[0] for v in weights]\n","\n","plt.figure(figsize=(15,10))\n","plt.xticks(range(0, len(x)), labels=labels, rotation=50)\n","plt.plot(x)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXStGLiZea5J"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"colab":{"name":"Twitter.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}