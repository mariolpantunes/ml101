{"cells":[{"cell_type":"markdown","metadata":{"id":"YBWPMQd2lOoy"},"source":["# Naive Bayes (Discrete)\n","\n","The idea of this project is to write a simple Naive Bayes model to predict if a SMS message is spam or not.\n","Let us derive the necessary probabilities.\n","Naive Bayes is a models that relies on the Bayes' theorem:\n","\n","$$\n","P(Y|X) = \\frac{P(X|Y)\\times P(Y)}{P(X)}\n","$$\n","\n","For this dataset we can write the equation as:\n","\n","$$\\begin{aligned}\n","P(y|W_1, ... W_n) &= \\frac{P(W_0, ... W_n|y)\\times P(y)}{P(W_1, ... W_n)} \\\\\n","P(y|W_1, ... W_n) &= \\frac{P(W_0 | W_1, ... W_n,y)\\times ...\\times P(y)}{P(W_0, ... W_n)} \\\\\n","P(y|W_1, ... W_n) &= \\frac{P(y) \\times \\prod_{i=0}^{n}P(W_i|y)}{P(W_0, ... W_n)} \\\\\n","P(y|W_1, ... W_n) &= \\frac{P(y) \\times \\prod_{i=0}^{n}P(W_i|y)}{P(y) \\times \\prod_{i=0}^{n}P(W_i|y) + P(\\neg y) \\times \\prod_{i=0}^{n}P(W_i|\\neg y)}\n","\\end{aligned}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8b2Zn7olOo3"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import nltk\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from wordcloud import WordCloud"]},{"cell_type":"markdown","metadata":{"id":"qUD1HzdOlOo4"},"source":["## Load the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"HgR9UTbclOo5"},"outputs":[],"source":["data = pd.read_csv('https://media.githubusercontent.com/media/mariolpantunes/ml101/main/datasets/spam.csv', encoding='latin-1')\n","data = data.rename(columns={\"Target\":\"label\", \"SMS\":\"sms\"})\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYPOU-QAlOo5"},"outputs":[],"source":["data['label'].value_counts().plot(kind='bar')"]},{"cell_type":"markdown","metadata":{"id":"-iJfAXuxlOo6"},"source":["## Preprocessing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE8UZwJVlOo6"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","ham_words = []\n","for sms in data[data['label'] == 'ham'].sms:\n","    text = sms.lower()\n","    tokens = nltk.word_tokenize(text)\n","    # remove stop words\n","    filtered_tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and len(w) > 2]\n","    # filter with lemmatizer\n","    filtered_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n","    ham_words.extend(filtered_tokens)\n","print(f'HAM {len(ham_words)}')\n","\n","spam_words = []\n","for sms in data[data['label'] == 'spam'].sms:\n","    text = sms.lower()\n","    tokens = nltk.word_tokenize(text)\n","    # remove stop words\n","    filtered_tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and len(w) > 2]\n","    # filter with lemmatizer\n","    filtered_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n","    spam_words.extend(filtered_tokens)\n","print(f'SPAM {len(spam_words)}')\n","\n","ham_string=(' ').join(ham_words)\n","ham_wordcloud = WordCloud(width=500, height=300).generate(ham_string)\n","\n","spam_string=(' ').join(spam_words)\n","spam_wordcloud = WordCloud(width=500, height=300).generate(spam_string)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwDOvdBklOo7"},"outputs":[],"source":["#Creating Ham wordcloud\n","plt.figure( figsize=(10,8), facecolor='g')\n","plt.imshow(ham_wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCdVqdbslOo8"},"outputs":[],"source":["#Spam Word cloud\n","plt.figure( figsize=(10,8), facecolor='w')\n","plt.imshow(spam_wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgqeaUJNlOo9"},"outputs":[],"source":["class NB:\n","    def __init__(self):\n","        self.p_spam = 0.0\n","        self.p_ham = 0.0\n","        self.spam_vocab = {}\n","        self.ham_vocab = {}\n","    \n","    def fit(self, ham_words, spam_words):\n","        total = len(ham_words)+len(spam_words)\n","        # compute prior\n","        \n","        # compute likelihood\n","        \n","    \n","    def predict(self, sms):\n","        text = sms.lower()\n","        tokens = nltk.word_tokenize(text)\n","        # remove stop words\n","        filtered_tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and len(w) > 2]\n","        # filter with lemmatizer\n","        filtered_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n","        \n","        prob_likelihood_spam = 1.0\n","        prob_likelihood_ham = 1.0\n","        \n","        prob = self.p_spam*prob_likelihood_spam / (self.p_spam*prob_likelihood_spam + self.p_ham*prob_likelihood_ham)\n","        if prob >= 0.5:\n","            return True\n","        else:\n","            return False\n","\n","    def __str__(self):\n","        return f'Prior ({self.p_ham}/{self.p_spam}) Vocab ({len(self.ham_vocab)}/{len(self.spam_vocab)})'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwzCgZgSlOo-"},"outputs":[],"source":["nb = NB()\n","nb.fit(ham_words, spam_words)\n","print(nb)\n","\n","print(nb.predict('Ok lar... Joking wif u oni...'))\n","print(nb.predict('Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&Cs apply 08452810075over18s'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOkAJ1CglOo-"},"outputs":[],"source":["data = data.replace(['ham','spam'],[False, True]) \n","results = data.apply(lambda row : nb.predict(row['sms']) == row['label'], axis = 1)\n","acc = results.value_counts()[True]/results.count()\n","print(acc)"]}],"metadata":{"interpreter":{"hash":"767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"00_naive_bays_00.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}